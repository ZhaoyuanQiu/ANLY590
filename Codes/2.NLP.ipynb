{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442a6ae4",
   "metadata": {},
   "source": [
    "## 1. Load Splitted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f13f0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_colwidth', None)\t\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir(r'D:\\ME\\GeorgeTown\\FALL2022\\ANLY-590\\ANLY590\\Data')\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de32a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in splitted data\n",
    "X_train=pd.read_csv(r'data_splitted\\X_train_splitted.csv')\n",
    "y_train=pd.read_csv(r'data_splitted\\y_train_splitted.csv')\n",
    "X_test=pd.read_csv(r'data_splitted\\X_test_splitted.csv')\n",
    "y_test=pd.read_csv(r'data_splitted\\y_test_splitted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02febec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DOUBLE FOAM TAPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Campur - Leher Kancing (DPT001-00) Batik karakter Alhadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                                                                                title  \n",
       "0                                                                           Paper Bag Victoria Secret  \n",
       "1                                        Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DOUBLE FOAM TAPE  \n",
       "2                                                         Maling TTS Canned Pork Luncheon Meat 397 gr  \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Campur - Leher Kancing (DPT001-00) Batik karakter Alhadi  \n",
       "4                                                                   Nescafe \\xc3\\x89clair Latte 220ml  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a29ae646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: (34085, 4)\n",
      "Test Size: (165, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Training Size:',X_train.shape)\n",
    "print('Test Size:',X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d2ae3",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a85e1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reference: https://towardsdatascience.com/cleaning-text-data-with-python-b69b47b97b76\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "# # In case of any corpus are missing \n",
    "stop_words = stopwords.words(\"english\")\n",
    "def text_preproc(x):\n",
    "    #lower case\n",
    "    x = x.lower()\n",
    "    #stopwords\n",
    "    x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    #Non-ascii character\n",
    "    x = x.encode('ascii', 'ignore').decode()\n",
    "    #URL\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    #Remove mentions\n",
    "    x = re.sub(r'@\\S+', ' ', x)\n",
    "    #Hash tag\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    #Remove ticks and the next character\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    #Remove punctuations\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    #Remove overspaces\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc040070",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['title'] = X_train['title'].apply(text_preproc)\n",
    "X_test['title'] = X_test['title'].apply(text_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67c8f88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>paper bag victoria secret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>double tape 3m vhb 12 mm x 4 5 original double foam tape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>maling tts canned pork luncheon meat 397 gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>daster batik lengan pendek motif acak campur leher kancing dpt001 00 batik karakter alhadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>nescafe xc3 x89clair latte 220ml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                                                                        title  \n",
       "0                                                                   paper bag victoria secret  \n",
       "1                                    double tape 3m vhb 12 mm x 4 5 original double foam tape  \n",
       "2                                                 maling tts canned pork luncheon meat 397 gr  \n",
       "3  daster batik lengan pendek motif acak campur leher kancing dpt001 00 batik karakter alhadi  \n",
       "4                                                            nescafe xc3 x89clair latte 220ml  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d2ae6",
   "metadata": {},
   "source": [
    "## 3.TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c56db3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_bow = TfidfVectorizer(encoding='utf-8',ngram_range=(1,2))\n",
    "tfidf_bow.fit(X_train['title'])\n",
    "X_train_tfidf=tfidf_bow.transform(X_train['title'])\n",
    "X_test_tfidf=tfidf_bow.transform(X_test['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b49a891c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34085, 155421)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbcee5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165, 155421)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93c7dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(r\"TFIDF\\X_train_tfidf.npz\",X_train_tfidf)\n",
    "scipy.sparse.save_npz(r\"TFIDF\\X_test_tfidf.npz\",X_test_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
